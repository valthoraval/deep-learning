{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVEEazxFZgr2",
        "toc": true
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Load-the-data\" data-toc-modified-id=\"1.-Load-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Load the data</a></span></li><li><span><a href=\"#2.-Filtering-out-the-noise\" data-toc-modified-id=\"2.-Filtering-out-the-noise-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. Filtering out the noise</a></span></li><li><span><a href=\"#3.-Even-better-filtering\" data-toc-modified-id=\"3.-Even-better-filtering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Even better filtering</a></span></li><li><span><a href=\"#4.-Term-frequency-times-inverse-document-frequency\" data-toc-modified-id=\"4.-Term-frequency-times-inverse-document-frequency-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. Term frequency times inverse document frequency</a></span></li><li><span><a href=\"#5.-Utility-function\" data-toc-modified-id=\"5.-Utility-function-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5. Utility function</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjpQefesZgr4"
      },
      "source": [
        "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en).\n",
        "\n",
        "License: CC-BY-SA-NC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1VXs8S8Zgr7"
      },
      "source": [
        "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Text data pre-processing</div>\n",
        "\n",
        "In this exercice, we shall load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers.\n",
        "\n",
        "\"What is there to pre-process?\" you might ask. Well, actually, text data comes in a very noisy form that we, humans, have become accustomed to and filter out effortlessly to grasp the core meaning of the text. It has a lot of formatting (fonts, colors, typography...), punctuation, abbreviations, common words, grammatical rules, etc. that we might wish to discard before even starting the data analysis.\n",
        "\n",
        "Here are some pre-processing steps that can be performed on text:\n",
        "1. loading the data, removing attachements, merging title and body;\n",
        "2. tokenizing - splitting the text into atomic \"words\";\n",
        "3. removal of stop-words - very common words;\n",
        "4. removal of non-words - punctuation, numbers, gibberish;\n",
        "3. lemmatization - merge together \"find\", \"finds\", \"finder\".\n",
        "\n",
        "The final goal is to be able to represent a document as a mathematical object, e.g. a vector, that our machine learning black boxes can process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGMK6qekZgr9"
      },
      "source": [
        "# 1. Text classification in English\n",
        "\n",
        "## 1.1 Load the data\n",
        "\n",
        "Let's first load the emails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "26qQSlBNZpZY",
        "outputId": "b2e8b085-3c02-4d95-985d-674f01bc1479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep-learning'...\n",
            "remote: Enumerating objects: 7274, done.\u001b[K\n",
            "remote: Counting objects: 100% (280/280), done.\u001b[K\n",
            "remote: Compressing objects: 100% (147/147), done.\u001b[K\n",
            "remote: Total 7274 (delta 137), reused 242 (delta 125), pack-reused 6994\u001b[K\n",
            "Receiving objects: 100% (7274/7274), 106.71 MiB | 16.93 MiB/s, done.\n",
            "Resolving deltas: 100% (3102/3102), done.\n",
            "Checking out files: 100% (6425/6425), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Installing collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/SupaeroDataScience/deep-learning\n",
        "!mv deep-learning/data .\n",
        "!mv deep-learning/NLP/datasets .\n",
        "!pip install nltk unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eYwNdKZiZgr9",
        "outputId": "248a7ed8-84b4-4bbf-8e40-6ab23c21bf09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of emails 2893\n",
            "email file: data/lingspam_public/bare/part8/6-829msg1.txt\n",
            "email is a spam: False\n",
            "Subject: genderless ?\n",
            "\n",
            "content - length : 350 in some languages which are standarly described as genderless it is still the case , i am pretty sure , that people say things like ' that woman ' instead of 's he ' and because this looks like a purely lexical matter , the appearance of genderlessness is preserved . i wonder if this applies to the languages recently described as genderless on this list .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "data_switch=1\n",
        "if(data_switch==0):\n",
        "    train_dir = 'data/ling-spam/train-mails/'\n",
        "    email_path = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
        "else:\n",
        "    train_dir = 'data/lingspam_public/bare/'\n",
        "    email_path = []\n",
        "    email_label = []\n",
        "    for d in os.listdir(train_dir):\n",
        "        folder = os.path.join(train_dir,d)\n",
        "        email_path += [os.path.join(folder,f) for f in os.listdir(folder)]\n",
        "        email_label += [f[0:3]=='spm' for f in os.listdir(folder)]\n",
        "print(\"number of emails\",len(email_path))\n",
        "email_nb = 8 # try 8 for a spam example\n",
        "print(\"email file:\", email_path[email_nb])\n",
        "print(\"email is a spam:\", email_label[email_nb])\n",
        "print(open(email_path[email_nb]).read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBPuFTw2ZgsA"
      },
      "source": [
        "## 1.2. Filtering out the noise\n",
        "\n",
        "One nice thing about scikit-learn is that is has lots of preprocessing utilities. Like [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for instance, that converts a collection of text documents to a matrix of token counts.\n",
        "\n",
        "- To remove stop-words, we set: `stop_words='english'`\n",
        "- To convert all words to lowercase: `lowercase=True`\n",
        "- The default tokenizer in scikit-learn removes punctuation and only keeps words of more than 2 letters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UKLxbfC-ZgsB"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "countvect = CountVectorizer(input='filename', stop_words='english', lowercase=True)\n",
        "word_count = countvect.fit_transform(email_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8eLR5hi3ZgsC",
        "outputId": "f8ee7912-fceb-4991-8528-177500e75507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 2893\n",
            "Number of words: 60618\n",
            "Document - words matrix: (2893, 60618)\n",
            "First words: ['00', '000', '0000', '00001', '00003000140', '00003003958', '00007', '0001', '00010', '00014', '0003', '00036', '000bp', '000s', '000yen', '001', '0010', '0010010034', '0011', '00133', '0014', '00170', '0019', '00198', '002', '002656', '0027', '003', '0030', '0031', '00333', '0037', '0039', '003n7', '004', '0041', '0044', '0049', '005', '0057', '006', '0067', '007', '00710', '0073', '0074', '00799', '008', '009', '00919680', '0094', '00a', '00am', '00arrival', '00b', '00coffee', '00congress', '00d', '00dinner', '00f', '00h', '00hfstahlke', '00i', '00j', '00l', '00m', '00p', '00pm', '00r', '00t', '00tea', '00the', '00uzheb', '01', '0100', '01003', '01006', '0104', '0106', '01075', '0108', '011', '0111', '0117', '0118', '01202', '01222', '01223', '01225', '01232', '01235', '01273', '013', '0131', '01334', '0135', '01364', '0139', '013953', '013a']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of documents:\", len(email_path))\n",
        "words = countvect.get_feature_names()\n",
        "print(\"Number of words:\", len(words))\n",
        "print(\"Document - words matrix:\", word_count.shape)\n",
        "print(\"First words:\", words[0:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H9GRQxXZgsC"
      },
      "source": [
        "## 1.3. Even better filtering\n",
        "\n",
        "That's already quite ok, but this pre-processing does not perform lemmatization, the list of stop-words could be better and we could wish to remove non-english words (misspelled, with numbers, etc.).\n",
        "\n",
        "A slightly better preprocessing uses the [Natural Language Toolkit](https://www.nltk.org/https://www.nltk.org/). The one below:\n",
        "- tokenizes;\n",
        "- removes punctuation;\n",
        "- removes stop-words;\n",
        "- removes non-English and misspelled words (optional);\n",
        "- removes 1-character words;\n",
        "- removes non-alphabetical words (numbers and codes essentially)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qbwk9Mp0ZgsD",
        "outputId": "80e3bee7-dc83-449d-b16d-3fedffd52a91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OLBm1kLIZgsE"
      },
      "outputs": [],
      "source": [
        "from nltk import wordpunct_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words\n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "class LemmaTokenizer(object):\n",
        "    def __init__(self, remove_non_words=True):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "        self.stopwords = set(stopwords.words('english'))\n",
        "        self.words = set(words.words())\n",
        "        self.remove_non_words = remove_non_words\n",
        "    def __call__(self, doc):\n",
        "        # tokenize words and punctuation\n",
        "        word_list = wordpunct_tokenize(doc)\n",
        "        # remove stopwords\n",
        "        word_list = [word for word in word_list if word not in self.stopwords]\n",
        "        # remove non words\n",
        "        if(self.remove_non_words):\n",
        "            word_list = [word for word in word_list if word in self.words]\n",
        "        # remove 1-character words\n",
        "        word_list = [word for word in word_list if len(word)>1]\n",
        "        # remove non alpha\n",
        "        word_list = [word for word in word_list if word.isalpha()]\n",
        "        return [self.wnl.lemmatize(t) for t in word_list]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXUNGD9AZgsG"
      },
      "source": [
        "The LemmaTokenizer defined above will be applied further in this example. The next step is to define the Count Vectorization pipeline using this Tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sa66gFMlZgsG"
      },
      "outputs": [],
      "source": [
        "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))\n",
        "bow = countvect.fit_transform(email_path)\n",
        "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IIKuiDWSZgsH",
        "scrolled": true,
        "outputId": "d08dcf61-feba-4429-a036-2925b707bf6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 2893\n",
            "Number of words: 14279\n",
            "Document - words matrix: (2893, 14279)\n",
            "First words: ['aa', 'aal', 'aba', 'aback', 'abacus', 'abandon', 'abandoned', 'abandonment', 'abbas', 'abbreviation', 'abdomen', 'abduction', 'abed', 'aberrant', 'aberration', 'abide', 'abiding', 'abigail', 'ability', 'ablative', 'ablaut', 'able', 'abler', 'aboard', 'abolition', 'abord', 'aboriginal', 'aborigine', 'abound', 'abox', 'abreast', 'abridged', 'abroad', 'abrogate', 'abrook', 'abruptly', 'abscissa', 'absence', 'absent', 'absolute', 'absolutely', 'absoluteness', 'absolutist', 'absolutive', 'absolutization', 'absorbed', 'absorption', 'abstract', 'abstraction', 'abstractly', 'abstractness', 'absurd', 'absurdity', 'abu', 'abundance', 'abundant', 'abuse', 'abusive', 'abyss', 'academe', 'academic', 'academically', 'academician', 'academy', 'accelerate', 'accelerated', 'accelerative', 'accent', 'accentuate', 'accentuation', 'accept', 'acceptability', 'acceptable', 'acceptance', 'acceptation', 'accepted', 'acception', 'access', 'accessibility', 'accessible', 'accessibly', 'accidence', 'accident', 'accidental', 'accidentality', 'accidentally', 'acclaim', 'accommodate', 'accommodation', 'accompany', 'accomplish', 'accomplished', 'accomplishment', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accountability', 'accountant']\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of documents:\", len(email_path))\n",
        "words = countvect.get_feature_names()\n",
        "print(\"Number of words:\", len(words))\n",
        "print(\"Document - words matrix:\", bow.shape)\n",
        "print(\"First words:\", words[0:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb-3Z74eZgsI"
      },
      "source": [
        "## 1.4. Using the bag of words (BOW) object to classify spam\n",
        "\n",
        "Let's start by splitting the data into train and test sets, using 20% of the data for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Gf-54hwSZgsI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(bow,email_label,test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rS7EDB-ZgsI"
      },
      "source": [
        "In this simple example we will use a Logistic Regression Classifier. Let's fit it to our Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BxD1T5OiZgsJ",
        "outputId": "b3e11ab1-13d1-44a0-af62-128a24a0c45d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.9861830742659758\n",
            "Precision : 1.0\n",
            "Recall : 0.9130434782608695\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "lr_classifier=LogisticRegression()\n",
        "lr_classifier.fit(X_train,y_train)\n",
        "\n",
        "y_predicted = lr_classifier.predict(X_test)\n",
        "\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
        "print(\"Precision :\",metrics.precision_score(y_test,y_predicted))\n",
        "print(\"Recall :\",metrics.recall_score(y_test,y_predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKsqSjg0ZgsJ"
      },
      "source": [
        "In many cases, Bag of Words can provide sufficient information for classification. In this case, the accuracy reached by our classifier is pretty good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naewZmg0ZgsK"
      },
      "source": [
        "## 1.5. Term frequency times inverse document frequency\n",
        "\n",
        "After this first preprocessing, each document is summarized by a vector of size \"number of words in the extracted dictionnary\". For example, the first email in the list has become:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s9imM6DqZgsK",
        "scrolled": false,
        "outputId": "be9a3a69-788b-4c08-b525-9e6ce537152e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original email:\n",
            "Subject: summary on english adjectives\n",
            "\n",
            "dear linguists , about a week ago i posted the following query . > i am working on english adjectives . are the following sentences > acceptable ? if acceptable , please mark them with a check . > if not acceptable , please mark them with a cross . if uncertain > or dubious , please mark them with a question mark . any > comment is welcome . > > ( 1 ) john was careful to lock the door . > ( 2 ) john was greedy to keep all the money to himself . > ( 3 ) john was worthy to be praised by them all . > ( 4 ) the place is convenient to visit . > ( 5 ) john is jealous that she succeeded . > ( 6 ) john was bored to hear her endless talk . > ( 7 ) john was confused to be told to do so many things at once . > ( 8 ) john was hurt to be insulted . > ( 9 ) i am eager that they should win . > ( 10 ) john was incredulous that mary put it into practice . > ( 11 ) mary is keen that we should go . > ( 12 ) it was heroic of them to oppose the invader . > ( 13 ) john was irresponsible to sabotage his duties . > ( 14 ) it is significant that they worked as volunteers . > ( 15 ) it is sufficient to give him some money . > ( 16 ) it is not suitable to dress casual at a wedding reception . i have received 17 replies . i would like to thank those who gave wme answers and comments . the following is the numbers of informants who indicated respective acceptability in each sentence . acceptable dubious / unacceptable uncertain ( 1 ) 16 1 0 ( 2 ) 7 4 6 ( 3 ) 7 7 3 ( 4 ) 15 1 1 ( 5 ) 13 3 1 ( 6 ) 8 2 7 ( 7 ) 6 3 8 ( 8 ) 7 2 8 ( 9 ) 11 4 2 ( 10 ) 15 2 0 ( 11 ) 11 3 3 ( 12 ) 16 0 1 ( 13 ) 6 3 8 ( 14 ) 17 0 0 ( 15 ) 16 1 0 ( 16 ) 10 1 6 part of discussion and analysis of this data will be presented by professor katsumasa yagi ( kwansei gakuin university , japan ) at the 24th lacus forum to be held at the university of york , toronto , ontario , canada on 2nd august and in his subsequent papers . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - atsuko umesaki ( tezukayama college ) e-mail : sgs03312 @ niftyserve . or . jp umesaki @ tezukayama-u . ac . jp - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\n",
            "Bag of words representation (83 words in dict):\n",
            "{'subject': 1, 'summary': 1, 'dear': 1, 'week': 1, 'ago': 1, 'posted': 1, 'following': 3, 'query': 1, 'working': 1, 'acceptable': 4, 'please': 3, 'mark': 4, 'check': 1, 'cross': 1, 'uncertain': 2, 'dubious': 2, 'question': 1, 'comment': 1, 'welcome': 1, 'careful': 1, 'lock': 1, 'door': 1, 'greedy': 1, 'keep': 1, 'money': 2, 'worthy': 1, 'place': 1, 'convenient': 1, 'visit': 1, 'jealous': 1, 'hear': 1, 'endless': 1, 'talk': 1, 'confused': 1, 'told': 1, 'many': 1, 'hurt': 1, 'eager': 1, 'win': 1, 'incredulous': 1, 'mary': 2, 'put': 1, 'practice': 1, 'keen': 1, 'go': 1, 'heroic': 1, 'oppose': 1, 'invader': 1, 'irresponsible': 1, 'sabotage': 1, 'significant': 1, 'worked': 1, 'sufficient': 1, 'give': 1, 'suitable': 1, 'dress': 1, 'casual': 1, 'wedding': 1, 'reception': 1, 'received': 1, 'would': 1, 'like': 1, 'thank': 1, 'gave': 1, 'respective': 1, 'acceptability': 1, 'sentence': 1, 'unacceptable': 1, 'part': 1, 'discussion': 1, 'analysis': 1, 'data': 1, 'professor': 1, 'yagi': 1, 'university': 2, 'japan': 1, 'forum': 1, 'york': 1, 'canada': 1, 'august': 1, 'subsequent': 1, 'college': 1, 'mail': 1}\n",
            "\n",
            "Vector reprensentation (83 non-zero elements):\n",
            "  (0, 12153)\t1\n",
            "  (0, 12266)\t1\n",
            "  (0, 3118)\t1\n",
            "  (0, 13983)\t1\n",
            "  (0, 316)\t1\n",
            "  (0, 9507)\t1\n",
            "  (0, 4923)\t3\n",
            "  (0, 10040)\t1\n",
            "  (0, 14171)\t1\n",
            "  (0, 72)\t4\n",
            "  (0, 9344)\t3\n",
            "  (0, 7588)\t4\n",
            "  (0, 2025)\t1\n",
            "  (0, 2968)\t1\n",
            "  (0, 13248)\t2\n",
            "  (0, 3860)\t2\n",
            "  (0, 10043)\t1\n",
            "  (0, 2359)\t1\n",
            "  (0, 13998)\t1\n",
            "  (0, 1812)\t1\n",
            "  (0, 7334)\t1\n",
            "  (0, 3763)\t1\n",
            "  (0, 5494)\t1\n",
            "  (0, 6877)\t1\n",
            "  (0, 7983)\t2\n",
            "  :\t:\n",
            "  (0, 10238)\t1\n",
            "  (0, 10232)\t1\n",
            "  (0, 14188)\t1\n",
            "  (0, 7224)\t1\n",
            "  (0, 12708)\t1\n",
            "  (0, 5235)\t1\n",
            "  (0, 10605)\t1\n",
            "  (0, 71)\t1\n",
            "  (0, 11235)\t1\n",
            "  (0, 13217)\t1\n",
            "  (0, 8911)\t1\n",
            "  (0, 3594)\t1\n",
            "  (0, 507)\t1\n",
            "  (0, 3091)\t1\n",
            "  (0, 9778)\t1\n",
            "  (0, 14216)\t1\n",
            "  (0, 13427)\t2\n",
            "  (0, 6774)\t1\n",
            "  (0, 5017)\t1\n",
            "  (0, 14253)\t1\n",
            "  (0, 1753)\t1\n",
            "  (0, 932)\t1\n",
            "  (0, 12182)\t1\n",
            "  (0, 2307)\t1\n",
            "  (0, 7486)\t1\n"
          ]
        }
      ],
      "source": [
        "mail_number = 0\n",
        "text = open(email_path[mail_number]).read()\n",
        "print(\"Original email:\")\n",
        "print(text)\n",
        "\n",
        "emailBagOfWords = {feat2word[i]: bow[mail_number, i] for i in bow[mail_number, :].nonzero()[1]}\n",
        "print(\"Bag of words representation (\", len(emailBagOfWords), \" words in dict):\", sep='')\n",
        "print(emailBagOfWords)\n",
        "print(\"\\nVector reprensentation (\", bow[mail_number, :].nonzero()[1].shape[0], \" non-zero elements):\", sep='')\n",
        "print(bow[mail_number, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCV1BC1RZgsL"
      },
      "source": [
        "Counting words is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
        "\n",
        "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
        "\n",
        "Another refinement on top of `tf` is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
        "\n",
        "This downscaling is called `tf–idf` for “Term Frequency times Inverse Document Frequency” and again, scikit-learn does the job for us with the [TfidfTransformer](scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pNuCQwo8ZgsL",
        "outputId": "9ee63494-08d5-4606-cb97-e68c3589d87f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2893, 14279)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf = TfidfTransformer().fit_transform(bow)\n",
        "tfidf.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhKhbIoeZgsM"
      },
      "source": [
        "Let's run the classification process again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-NkGrarpZgsM",
        "outputId": "9427b439-f328-4e25-80e4-ce047e2e825e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.9740932642487047\n",
            "Precision : 1.0\n",
            "Recall : 0.84375\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tfidf,email_label,test_size=0.2)\n",
        "\n",
        "#Fitting classifier\n",
        "lr_classifier.fit(X_train,y_train)\n",
        "\n",
        "#Testing classifier\n",
        "y_predicted = lr_classifier.predict(X_test)\n",
        "\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
        "print(\"Precision :\",metrics.precision_score(y_test,y_predicted))\n",
        "print(\"Recall :\",metrics.recall_score(y_test,y_predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_icBmOuZgsM"
      },
      "source": [
        "In this simplae case, additional filtering is unecessary and even removed some information. There is indeed likely a link between the abundance of words/long emails and the fact that this email is a spam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcZVAEwsZgsN"
      },
      "source": [
        "# 2. Text classification in French\n",
        "\n",
        "The previously used dataset is a widely used dataset for introductory text classification. \n",
        "\n",
        "The field of Natural Language Understanding, and Natural Language Classification in particular, suffers from two challenges :\n",
        "- Adapting the features and methodologies to various and more complex datasets\n",
        "- Adapting the process to languages other than english\n",
        "\n",
        "Concerning the latter, one has to take into account that most of NLU research is currently performed on english. Datasets are rarely available for other languages, and the algorithms proposed for better NLU are often left untested on foreign data. \n",
        "French, for instance, has less efficient lemmatization (french is a richly flected language). In the following section, we will reuse the same methodologies on a french dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4TzD5FKrZgsN",
        "outputId": "97383a46-533d-4552-978c-1fb698ba694d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                     game_fr                      game_en  \\\n",
              "0           1     .hack//G.U. Last Recode      .hack//G.U. Last Recode   \n",
              "1           2  .hack//G.U. Vol.1//Rebirth  .hack//G.U. Vol. 1//Rebirth   \n",
              "2           3     .hack//Infection Part 1     .hack//Infection: Part 1   \n",
              "3           4      .hack//Mutation Part 2      .hack//Mutation: Part 2   \n",
              "4           5      .hack//Outbreak Part 3      .hack//Outbreak: Part 3   \n",
              "\n",
              "  platform website_rating public_rating         publishor/developer  \\\n",
              "0      PS4          14/20         --/20  Bandai Namco Entertainment   \n",
              "1      PS2          15/20       18.2/20  Bandai Namco CyberConnect2   \n",
              "2      PS2          15/20       15.1/20        CyberConnect2 Bandai   \n",
              "3      PS2          14/20       16.4/20        Bandai CyberConnect2   \n",
              "4      PS2          13/20       15.3/20         CyberConnect2 Atari   \n",
              "\n",
              "                   release type classification  \\\n",
              "0         03 Novembre 2017  RPG        +12 ans   \n",
              "1  Date de sortie inconnue  RPG         +7 ans   \n",
              "2             26 Mars 2004  RPG        +12 ans   \n",
              "3             18 Juin 2004  RPG        +12 ans   \n",
              "4        17 Septembre 2004  RPG        +12 ans   \n",
              "\n",
              "                                                 url  \\\n",
              "0      http://www.jeuxvideo.com/jeux/ps4/jeu-674262/   \n",
              "1  http://www.jeuxvideo.com/jeux/playstation-2-ps...   \n",
              "2  http://www.jeuxvideo.com/jeux/playstation-2-ps...   \n",
              "3  http://www.jeuxvideo.com/jeux/playstation-2-ps...   \n",
              "4  http://www.jeuxvideo.com/jeux/playstation-2-ps...   \n",
              "\n",
              "                                         description  \n",
              "0  Au contraire d’autres titres, None ,''.hack'' ...  \n",
              "1   Avec plus de 20 œuvres de fiction sur de mult...  \n",
              "2   S'appuyant sur la maxime « Mieux vaut tard qu...  \n",
              "3   Voici enfin le second volet de la quadrilogie...  \n",
              "4   Comme la maxime «Jamais deux sans trois» ne c...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae487d40-b008-4768-a703-d2d24045bba9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>game_fr</th>\n",
              "      <th>game_en</th>\n",
              "      <th>platform</th>\n",
              "      <th>website_rating</th>\n",
              "      <th>public_rating</th>\n",
              "      <th>publishor/developer</th>\n",
              "      <th>release</th>\n",
              "      <th>type</th>\n",
              "      <th>classification</th>\n",
              "      <th>url</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>.hack//G.U. Last Recode</td>\n",
              "      <td>.hack//G.U. Last Recode</td>\n",
              "      <td>PS4</td>\n",
              "      <td>14/20</td>\n",
              "      <td>--/20</td>\n",
              "      <td>Bandai Namco Entertainment</td>\n",
              "      <td>03 Novembre 2017</td>\n",
              "      <td>RPG</td>\n",
              "      <td>+12 ans</td>\n",
              "      <td>http://www.jeuxvideo.com/jeux/ps4/jeu-674262/</td>\n",
              "      <td>Au contraire d’autres titres, None ,''.hack'' ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>.hack//G.U. Vol.1//Rebirth</td>\n",
              "      <td>.hack//G.U. Vol. 1//Rebirth</td>\n",
              "      <td>PS2</td>\n",
              "      <td>15/20</td>\n",
              "      <td>18.2/20</td>\n",
              "      <td>Bandai Namco CyberConnect2</td>\n",
              "      <td>Date de sortie inconnue</td>\n",
              "      <td>RPG</td>\n",
              "      <td>+7 ans</td>\n",
              "      <td>http://www.jeuxvideo.com/jeux/playstation-2-ps...</td>\n",
              "      <td>Avec plus de 20 œuvres de fiction sur de mult...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>.hack//Infection Part 1</td>\n",
              "      <td>.hack//Infection: Part 1</td>\n",
              "      <td>PS2</td>\n",
              "      <td>15/20</td>\n",
              "      <td>15.1/20</td>\n",
              "      <td>CyberConnect2 Bandai</td>\n",
              "      <td>26 Mars 2004</td>\n",
              "      <td>RPG</td>\n",
              "      <td>+12 ans</td>\n",
              "      <td>http://www.jeuxvideo.com/jeux/playstation-2-ps...</td>\n",
              "      <td>S'appuyant sur la maxime « Mieux vaut tard qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>.hack//Mutation Part 2</td>\n",
              "      <td>.hack//Mutation: Part 2</td>\n",
              "      <td>PS2</td>\n",
              "      <td>14/20</td>\n",
              "      <td>16.4/20</td>\n",
              "      <td>Bandai CyberConnect2</td>\n",
              "      <td>18 Juin 2004</td>\n",
              "      <td>RPG</td>\n",
              "      <td>+12 ans</td>\n",
              "      <td>http://www.jeuxvideo.com/jeux/playstation-2-ps...</td>\n",
              "      <td>Voici enfin le second volet de la quadrilogie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>.hack//Outbreak Part 3</td>\n",
              "      <td>.hack//Outbreak: Part 3</td>\n",
              "      <td>PS2</td>\n",
              "      <td>13/20</td>\n",
              "      <td>15.3/20</td>\n",
              "      <td>CyberConnect2 Atari</td>\n",
              "      <td>17 Septembre 2004</td>\n",
              "      <td>RPG</td>\n",
              "      <td>+12 ans</td>\n",
              "      <td>http://www.jeuxvideo.com/jeux/playstation-2-ps...</td>\n",
              "      <td>Comme la maxime «Jamais deux sans trois» ne c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae487d40-b008-4768-a703-d2d24045bba9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae487d40-b008-4768-a703-d2d24045bba9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae487d40-b008-4768-a703-d2d24045bba9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#load video games reviews\n",
        "vgr = pd.read_csv(\"datasets/jvc.csv\")\n",
        "vgr.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "17I9wrn8ZgsN",
        "outputId": "987008eb-740e-488a-d31d-f91e084834a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f326eca6d90>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#convert rating to numeric values, and plot the histogram of values\n",
        "rating=vgr.website_rating.apply(lambda k: k[:-3])\n",
        "vgr['rating']=pd.to_numeric(rating)\n",
        "vgr.rating.plot.hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB1HYixvZgsO"
      },
      "source": [
        "Most games seem to have a rating between 11 and 16. In this exercise, we will try to determine if we can determine if a game is very good (rating above 16) or very bad (rating below 11) based only on the summary of its review.\n",
        "\n",
        "Let's start by splitting the dataset between good and bad games"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nZWhweYuZgsO",
        "outputId": "f3fbd325-0000-47b0-fdb5-8849d9ab2cd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "bad=vgr[(vgr.rating<=11) & (vgr.platform==\"PC\")]\n",
        "bad['quality']=pd.Series([\"bad\"]*len(bad.index),index=bad.index)\n",
        "good=vgr[(vgr.rating>=16) & (vgr.platform==\"PC\")]\n",
        "good['quality']=pd.Series([\"good\"]*len(good.index),index=good.index)\n",
        "selected_games=pd.concat([good,bad]).dropna()\n",
        "\n",
        "#Keep only reviews and \n",
        "game_reviews=selected_games['description']\n",
        "game_quality=selected_games['quality']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRuc6EaNZgsO"
      },
      "source": [
        "Lemmatization in French is a tricky issue.\n",
        "\n",
        "One example : the verb finir can be expressed as finissons, finirez, finisse, finit, etc...\n",
        "Lemmatization is typically less efficient in french than in english. \n",
        "\n",
        "Another alternative is to use Stemming instead. Stemming uses RegEx rules to truncate the end of a word that would normally correspond to conjugations, inflections, etc...\n",
        "Stemming destructs the readability of the words by truncating their end, but runs faster than Lemmatization\n",
        "\n",
        "In the next cell, we adapt the LemmaTokenizer that we defined earlier using a FrenchStemmer instead. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vA9TEMYXZgsO"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.snowball import FrenchStemmer\n",
        "from nltk import wordpunct_tokenize          \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words\n",
        "from string import punctuation\n",
        "class FrenchStemTokenizer(object):\n",
        "    def __init__(self, remove_non_words=True):\n",
        "        self.st = FrenchStemmer()\n",
        "        self.stopwords = set(stopwords.words('french'))\n",
        "        self.words = set(words.words())\n",
        "        self.remove_non_words = remove_non_words\n",
        "    def __call__(self, doc):\n",
        "        # tokenize words and punctuation\n",
        "        word_list = wordpunct_tokenize(doc)\n",
        "        # remove stopwords\n",
        "        word_list = [word for word in word_list if word not in self.stopwords]\n",
        "        # remove non words\n",
        "        if(self.remove_non_words):\n",
        "            word_list = [word for word in word_list if word in self.words]\n",
        "        # remove 1-character words\n",
        "        word_list = [word for word in word_list if len(word)>1]\n",
        "        # remove non alpha\n",
        "        word_list = [word for word in word_list if word.isalpha()]\n",
        "        return [self.st.stem(t) for t in word_list]\n",
        "\n",
        "countvect = CountVectorizer(tokenizer=FrenchStemTokenizer(remove_non_words=True))\n",
        "bow_games = countvect.fit_transform(game_reviews)\n",
        "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eirJF0KjZgsP"
      },
      "source": [
        "### Classify with BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lbgEZqdpZgsP",
        "outputId": "708a2654-9ae8-4be0-a446-ee09aa7ee082",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 2349\n",
            "Number of words: 3161\n",
            "Document - words matrix: (2349, 3161)\n",
            "First words: ['abandon', 'abomin', 'abord', 'abras', 'absenc', 'absolut', 'absorb', 'abstract', 'abyssal', 'academy', 'accent', 'accept', 'access', 'accessibl', 'acclaim', 'accord', 'accouch', 'accru', 'accus', 'ace', 'aci', 'acolyt', 'acquisit', 'acquit', 'acquitt', 'act', 'action', 'activ', 'activity', 'actual', 'ad', 'adag', 'adapt', 'add', 'addict', 'addit', 'adieu', 'adieux', 'admir', 'adolescent', 'adopt', 'ador', 'adrenalin', 'adroit', 'advanc', 'advanced', 'adventur', 'advers', 'aero', 'affect', 'affili', 'affirm', 'affluenc', 'afflux', 'affront', 'afraid', 'after', 'afterbirth', 'against', 'age', 'agend', 'agent', 'aggress', 'agit', 'agon', 'agricol', 'agricultur', 'ah', 'aid', 'aiguill', 'aim', 'aion', 'air', 'al', 'alan', 'album', 'alfa', 'ali', 'alien', 'align', 'aliment', 'aliv', 'all', 'allan', 'allemand', 'aller', 'allianc', 'allur', 'allus', 'almost', 'alon', 'alpha', 'alphabet', 'altern', 'am', 'amass', 'amateur', 'amatric', 'ambit', 'ambivalent']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of documents:\", len(game_reviews))\n",
        "words = countvect.get_feature_names()\n",
        "print(\"Number of words:\", len(words))\n",
        "print(\"Document - words matrix:\", bow_games.shape)\n",
        "print(\"First words:\", words[0:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "54z4k0VpZgsP",
        "outputId": "d43f5f93-3e4e-49dc-b303-d5f6dc215a68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.6510638297872341\n",
            "Precision : 0.6224066390041494\n",
            "Recall : 0.672645739910314\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(bow_games,game_quality,test_size=0.2)\n",
        "\n",
        "#Fitting classifier\n",
        "lr_classifier.fit(X_train,y_train)\n",
        "\n",
        "#Testing classifier\n",
        "y_predicted = lr_classifier.predict(X_test)\n",
        "\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
        "print(\"Precision :\",metrics.precision_score(y_test,y_predicted,pos_label=\"good\"))\n",
        "print(\"Recall :\",metrics.recall_score(y_test,y_predicted,pos_label=\"good\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLqa6tFTZgsQ"
      },
      "source": [
        "### Classify using tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "hoDUgqKgZgsQ",
        "outputId": "c2984dfc-047a-4b70-9fad-88e0fe9cdd75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2349, 3161)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_games = TfidfTransformer().fit_transform(bow_games)\n",
        "tfidf_games.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bLCm-xlzZgsQ",
        "outputId": "04d7de21-2911-43c3-e721-523426e95e15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.7191489361702128\n",
            "Precision : 0.7127659574468085\n",
            "Recall : 0.7976190476190477\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_games,game_quality,test_size=0.2)\n",
        "\n",
        "#Fitting classifier\n",
        "lr_classifier.fit(X_train,y_train)\n",
        "\n",
        "#Testing classifier\n",
        "y_predicted = lr_classifier.predict(X_test)\n",
        "\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
        "print(\"Precision :\",metrics.precision_score(y_test,y_predicted,pos_label=\"good\"))\n",
        "print(\"Recall :\",metrics.recall_score(y_test,y_predicted,pos_label=\"good\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYmgT33VZgsR"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "up7NxbxnZgsR"
      },
      "source": [
        "Tfidf and Bow are usually very efficient features to manipulate during cassification. However, please note that their size is directly related to the size of the vocabulary of our corpus. \n",
        "\n",
        "In our current example, even when using stemming, the dimensionnality of our bow or tfidf vectors is still very high (3161). This is not maintaintable with increasing corpora sizes.\n",
        "\n",
        "In this sction, we will use the word2vec embeddings, that address this issue by proposing an architecture that learns individual representations for words in a vector space of given dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "L38iGShaZgsS"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.snowball import FrenchStemmer\n",
        "from nltk import wordpunct_tokenize          \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words\n",
        "from string import punctuation\n",
        "import unidecode\n",
        "\n",
        "class FrenchTokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.stopwords = set(stopwords.words('french'))\n",
        "        self.words = set(words.words())\n",
        "    def __call__(self, doc):\n",
        "        # tokenize words and punctuation\n",
        "        word_list = wordpunct_tokenize(doc)\n",
        "        # remove stopwords\n",
        "        word_list = [word for word in word_list if word not in self.stopwords]\n",
        "        # remove 1-character words\n",
        "        word_list = [word for word in word_list if len(word)>1]\n",
        "        # remove non alpha\n",
        "        word_list = [word for word in word_list if word.isalpha()]\n",
        "        return [unidecode.unidecode(t) for t in word_list]\n",
        "\n",
        "tok=FrenchTokenizer()\n",
        "\n",
        "text_for_word2vec=[tok(sent) for sent in game_reviews]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRaGsoecZgsS"
      },
      "source": [
        "The operation above will tokenize all texts by keeping stemmed tokens. Please note the following choices :\n",
        "- we have applied stemming in order to reduce the dimensionality of our feature space\n",
        "- we have removed stop words, in order to not let context be learned with it. (depending on the use case, you may want to keep them or remove them)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOWsPngIZgsS"
      },
      "source": [
        "We can now train the Word2Vec model :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XliGqhWMZgsT"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model=Word2Vec(text_for_word2vec,size=200,window=5,min_count=1)\n",
        "model.save(\"word2vec.model\")\n",
        "w2v=dict(zip(model.wv.index2word, model.wv.vectors))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXaVjIVMZgsT"
      },
      "source": [
        "Let's check word similarity in our trained data :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "M0y6VW1dZgsT",
        "outputId": "3bfb3189-4d03-4af2-c738-fc068066d030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('titre', 0.999985933303833),\n",
              " ('monde', 0.9999849796295166),\n",
              " ('jeux', 0.9999842643737793),\n",
              " ('tout', 0.9999830722808838),\n",
              " ('faire', 0.9999827146530151),\n",
              " ('Le', 0.9999825358390808),\n",
              " ('donc', 0.9999821186065674),\n",
              " ('encore', 0.9999820590019226),\n",
              " ('comme', 0.9999817609786987),\n",
              " ('studio', 0.9999815821647644)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "model.wv.most_similar(positive=\"jeu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDKaY4dJZgsU"
      },
      "source": [
        "Let's now try again to classify our samples using these embddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZVj4nK80ZgsU"
      },
      "outputs": [],
      "source": [
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self,word2vec,dim):\n",
        "        self.word2vec=word2vec\n",
        "        self.dim=dim\n",
        "        \n",
        "    def fit(self,X,y):\n",
        "        return self\n",
        "    \n",
        "    def transform(self,X):\n",
        "        return np.array([\n",
        "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "i_n-EJW5ZgsV",
        "outputId": "314aa183-0adf-41b6-b651-918d2075a6c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer',\n",
              "                 <__main__.MeanEmbeddingVectorizer object at 0x7f32666f6b50>),\n",
              "                ('classifier', LogisticRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(game_reviews,game_quality,test_size=0.2)\n",
        "\n",
        "pipe=Pipeline([('vectorizer',MeanEmbeddingVectorizer(w2v,200)),('classifier',lr_classifier)])\n",
        "\n",
        "pipe.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Tg9TYPRdZgsV",
        "outputId": "7f1adb5b-20e0-4da2-f079-2cfdabcee081",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.5255319148936171\n",
            "Precision : 0.5255319148936171\n",
            "Recall : 1.0\n"
          ]
        }
      ],
      "source": [
        "predicted = pipe.predict(X_test)\n",
        "\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test,predicted))\n",
        "print(\"Precision :\",metrics.precision_score(y_test,predicted,pos_label=\"good\"))\n",
        "print(\"Recall :\",metrics.recall_score(y_test,predicted,pos_label=\"good\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNCJSWtaZgsW"
      },
      "source": [
        "What we observe here is that word2vec embeddings perform worse than what we learned from BOW or TFIDF. \n",
        "\n",
        "In our case, the training corpus for the embeddings was not large enough to ensure proper convergence and representation of the words.\n",
        "\n",
        "It is also common that for smaller corpora (<10.000 docs approximately), TFIDF usually performs better for classification, whereas Word2Vec produces better results with larger corpora and across domains (e.g. training on data from Wikipedia, and then using the vectors on data from another field)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4YIg9PaX4WdL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}